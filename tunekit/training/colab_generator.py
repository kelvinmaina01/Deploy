"""
Colab Notebook Generator
========================
Generates Jupyter notebooks for training SLMs on Google Colab.
No external dependencies required - notebooks are just JSON!
"""

import json
import base64
from typing import Dict, List


def new_markdown_cell(source: str) -> dict:
    """Create a markdown cell."""
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": source.split('\n') if isinstance(source, str) else source
    }


def new_code_cell(source: str) -> dict:
    """Create a code cell."""
    return {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": source.split('\n') if isinstance(source, str) else source
    }


def get_target_modules(model_id: str) -> list:
    """Get LoRA target modules based on model architecture."""
    model_id_lower = model_id.lower()
    
    if "phi" in model_id_lower:
        return ["q_proj", "k_proj", "v_proj", "o_proj", "fc1", "fc2"]
    else:
        # Llama, Mistral, Gemma, Qwen - standard transformer
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]


def estimate_training_time(num_examples: int, model_size: str) -> str:
    """Estimate training time based on dataset size and model."""
    # Rough estimates for T4 GPU on Colab
    base_minutes = {
        "2B": 2,
        "3B": 3,
        "3.8B": 4,
        "7B": 8,
    }
    
    base = base_minutes.get(model_size, 4)
    # Scale by dataset size (per 100 examples)
    minutes = base * (num_examples / 100) * 3  # 3 epochs
    
    if minutes < 5:
        return "~5 minutes"
    elif minutes < 30:
        return f"~{int(minutes)} minutes"
    else:
        hours = minutes / 60
        return f"~{hours:.1f} hours"


def generate_training_notebook(
    dataset_jsonl: str,
    model_id: str,
    model_name: str,
    analysis: dict
) -> str:
    """
    Generate a Jupyter notebook for training on Google Colab.
    
    Args:
        dataset_jsonl: User's JSONL data as string
        model_id: HuggingFace model ID (e.g., "microsoft/Phi-4-mini-instruct")
        model_name: Display name (e.g., "Phi-4 Mini")
        analysis: Dict from analyze.py with task_type, num_examples, etc.
    
    Returns:
        String with notebook content in JSON format
    """
    
    # Extract analysis info
    num_examples = analysis.get("num_examples", 0)
    task_type = analysis.get("task_type", "chat")
    model_size = analysis.get("model_size", "3B")
    
    # Get target modules for this model
    target_modules = get_target_modules(model_id)
    
    # Estimate training time
    est_time = estimate_training_time(num_examples, model_size)
    
    # Encode the JSONL data as base64 to avoid escaping issues
    dataset_b64 = base64.b64encode(dataset_jsonl.encode()).decode()
    
    # Build notebook structure
    notebook = {
        "nbformat": 4,
        "nbformat_minor": 0,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.10.0"
            },
            "accelerator": "GPU",
            "gpuClass": "standard",
            "colab": {
                "provenance": [],
                "gpuType": "T4"
            }
        },
        "cells": []
    }
    
    cells = []
    
    # =========================================================================
    # Cell 1: Title & Info (Markdown)
    # =========================================================================
    title_md = f"""# ðŸŽ¯ Fine-Tune {model_name}

## Dataset Overview
| Metric | Value |
|--------|-------|
| **Model** | {model_name} (`{model_id}`) |
| **Examples** | {num_examples:,} conversations |
| **Task Type** | {task_type.title()} |
| **Estimated Time** | {est_time} |

## Before You Start
1. **GPU Required**: Go to `Runtime` â†’ `Change runtime type` â†’ Select **T4 GPU**
2. **Run All Cells**: Click `Runtime` â†’ `Run all` or press `Ctrl+F9`
3. **Download Model**: After training, download your fine-tuned model from the Files panel

---
*Generated by [TuneKit](https://github.com/yourusername/tunekit) â€¢ Fine-tuning made simple*
"""
    cells.append(new_markdown_cell(title_md))
    
    # =========================================================================
    # Cell 2: Install Dependencies
    # =========================================================================
    install_code = """# Install required packages (takes ~2 minutes)
!pip install -q transformers accelerate peft bitsandbytes datasets trl

# Verify GPU
import torch
print(f"âœ“ PyTorch {torch.__version__}")
print(f"âœ“ CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸ No GPU detected! Go to Runtime â†’ Change runtime type â†’ GPU")
"""
    cells.append(new_code_cell(install_code))
    
    # =========================================================================
    # Cell 3: Embed Dataset
    # =========================================================================
    dataset_code = f'''# Load your training dataset
import base64
import json

# Your dataset (embedded from TuneKit)
DATASET_B64 = """{dataset_b64}"""

# Decode and parse
dataset_jsonl = base64.b64decode(DATASET_B64).decode()
conversations = [json.loads(line) for line in dataset_jsonl.strip().split("\\n") if line.strip()]

print(f"âœ“ Loaded {{len(conversations):,}} conversations")
print(f"\\nðŸ“ Sample conversation:")
if conversations:
    sample = conversations[0]
    for msg in sample.get("messages", [])[:3]:
        role = msg.get("role", "unknown")
        content = msg.get("content", "")[:100]
        print(f"  {{role}}: {{content}}...")
'''
    cells.append(new_code_cell(dataset_code))
    
    # =========================================================================
    # Cell 4: Load Model with 4-bit Quantization
    # =========================================================================
    load_model_code = f'''# Load model with 4-bit quantization for memory efficiency
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

MODEL_ID = "{model_id}"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print(f"Loading {{MODEL_ID}}...")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# Prepare for training
model = prepare_model_for_kbit_training(model)
model.config.use_cache = False

# LoRA configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules={json.dumps(target_modules)},
    bias="none",
    task_type="CAUSAL_LM",
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Print trainable parameters
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())
print(f"âœ“ Model loaded!")
print(f"âœ“ Trainable parameters: {{trainable:,}} / {{total:,}} ({{100*trainable/total:.2f}}%)")
'''
    cells.append(new_code_cell(load_model_code))
    
    # =========================================================================
    # Cell 5: Prepare Dataset
    # =========================================================================
    prepare_data_code = '''# Prepare dataset for training
from datasets import Dataset

def format_conversation(example):
    """Format conversation using chat template."""
    messages = example.get("messages", [])
    
    # Apply chat template if available
    if hasattr(tokenizer, "apply_chat_template"):
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    else:
        # Fallback formatting
        text = ""
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "system":
                text += f"System: {content}\\n"
            elif role == "user":
                text += f"User: {content}\\n"
            elif role == "assistant":
                text += f"Assistant: {content}\\n"
    
    return {"text": text}

# Create dataset
dataset = Dataset.from_list(conversations)
dataset = dataset.map(format_conversation)

# Tokenize
def tokenize(example):
    result = tokenizer(
        example["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized = dataset.map(tokenize, remove_columns=dataset.column_names)

# Split train/eval
split = tokenized.train_test_split(test_size=0.1, seed=42)
train_dataset = split["train"]
eval_dataset = split["test"]

print(f"âœ“ Training examples: {len(train_dataset):,}")
print(f"âœ“ Evaluation examples: {len(eval_dataset):,}")
'''
    cells.append(new_code_cell(prepare_data_code))
    
    # =========================================================================
    # Cell 6: Train
    # =========================================================================
    train_code = '''# Train the model
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    report_to="none",
    optim="paged_adamw_8bit",
)

# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

print("ðŸš€ Starting training...")
print("=" * 50)

# Train!
trainer.train()

print("=" * 50)
print("âœ“ Training complete!")
'''
    cells.append(new_code_cell(train_code))
    
    # =========================================================================
    # Cell 7: Save Model
    # =========================================================================
    model_slug = model_name.lower().replace(" ", "_").replace("-", "_")
    save_code = f'''# Save the fine-tuned model
import shutil
import os

OUTPUT_DIR = "./fine_tuned_{model_slug}"

# Save LoRA adapter
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"âœ“ Model saved to {{OUTPUT_DIR}}")

# Create zip for easy download
ZIP_NAME = f"{{OUTPUT_DIR}}.zip"
shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)

# Get file size
size_mb = os.path.getsize(ZIP_NAME) / (1024 * 1024)
print(f"âœ“ Created {{ZIP_NAME}} ({{size_mb:.1f}} MB)")
print()
print("ðŸ“¥ To download: Click the folder icon (left sidebar) â†’ Right-click the .zip â†’ Download")
'''
    cells.append(new_code_cell(save_code))
    
    # =========================================================================
    # Cell 8: Test Inference
    # =========================================================================
    test_code = '''# Test your fine-tuned model
def generate_response(prompt, max_new_tokens=256):
    """Generate a response from the fine-tuned model."""
    messages = [{"role": "user", "content": prompt}]
    
    if hasattr(tokenizer, "apply_chat_template"):
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        text = f"User: {prompt}\\nAssistant:"
    
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extract just the assistant's response
    if "Assistant:" in response:
        response = response.split("Assistant:")[-1].strip()
    return response

# Test it!
test_prompt = "Hello! Can you help me?"
print(f"ðŸ“ Prompt: {test_prompt}")
print(f"ðŸ¤– Response: {generate_response(test_prompt)}")
'''
    cells.append(new_code_cell(test_code))
    
    # =========================================================================
    # Cell 9: Next Steps (Markdown)
    # =========================================================================
    next_steps_md = f"""---

## ðŸŽ‰ What's Next?

Your fine-tuned **{model_name}** model is ready! Here's how to use it:

### Option 1: Use in Python
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("{model_id}")
tokenizer = AutoTokenizer.from_pretrained("{model_id}")

# Load your LoRA adapter
model = PeftModel.from_pretrained(base_model, "./fine_tuned_{model_slug}")
```

### Option 2: Merge & Export
```python
# Merge LoRA weights into base model
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./merged_model")
```

### Option 3: Upload to HuggingFace Hub
```python
model.push_to_hub("your-username/your-model-name")
tokenizer.push_to_hub("your-username/your-model-name")
```

---
*Happy fine-tuning! ðŸš€*
"""
    cells.append(new_markdown_cell(next_steps_md))
    
    # Add all cells to notebook
    notebook["cells"] = cells
    
    # Return as JSON string
    return json.dumps(notebook, indent=2)


def save_notebook(notebook_content: str, output_path: str) -> str:
    """Save notebook content to a file."""
    with open(output_path, 'w') as f:
        f.write(notebook_content)
    return output_path
